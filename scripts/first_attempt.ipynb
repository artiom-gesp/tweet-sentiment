{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos = pd.read_table('../data/train_pos.txt', header=None, names=['tweet'])\n",
    "df_train_neg = pd.read_table('../data/train_neg.txt', header=None, names=['tweet'])\n",
    "df_test = pd.read_table('../data/test_data.txt', header=None, names=['tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pos['sentiment'] = 1\n",
    "df_train_neg['sentiment'] = 0\n",
    "\n",
    "df_train = pd.concat([df_train_pos, df_train_neg])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy pipeline\n",
    "# English pipeline optimized for CPU. \n",
    "# Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer.\n",
    "# https://spacy.io/models/en\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# punctuation and stopwords\n",
    "punctuations = string.punctuation\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def tweet_cleaner(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.lemma_ != '-PRON-':\n",
    "            temp = token.lemma_.lower().strip()\n",
    "        else:\n",
    "            temp = token.lower_\n",
    "        tokens.append(temp)\n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        if (token not in punctuations) and (token not in stop_words):\n",
    "            clean_tokens.append(token)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        \"\"\"Override the transform method to clean text\"\"\"\n",
    "        collector = []\n",
    "        for text in tqdm(X, total=len(X), desc='Cleaning text:\\t'):\n",
    "            collector.append(clean_text(text))\n",
    "        return collector\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# basic function to clean the text\n",
    "def clean_text(text):\n",
    "    \"\"\"Removing spaces and converting the text into lowercase\"\"\"\n",
    "    return text.strip().lower()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different vectorizers\n",
    "bow_vector = CountVectorizer(tokenizer=tweet_cleaner, ngram_range=(1,1))\n",
    "tfidf_vector = TfidfVectorizer(tokenizer=tweet_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dimension: (137879,)\n",
      "y_train dimension: (137879,)\n",
      "X_test dimension: (59091,)\n",
      "y_train dimension: (59091,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_train['tweet']\n",
    "y = df_train['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "print(f'X_train dimension: {X_train.shape}')\n",
    "print(f'y_train dimension: {y_train.shape}')\n",
    "print(f'X_test dimension: {X_test.shape}')\n",
    "print(f'y_train dimension: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning text:\t: 100%|██████████| 137/137 [00:00<00:00, 122939.59it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\ricof\\.virtualenvs\\cil-sentiment-pXwVpgIY\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69247731\n",
      "Iteration 2, loss = 0.66372001\n",
      "Iteration 3, loss = 0.63898151\n",
      "Iteration 4, loss = 0.61510484\n",
      "Iteration 5, loss = 0.58993782\n",
      "Iteration 6, loss = 0.56279569\n",
      "Iteration 7, loss = 0.53407789\n",
      "Iteration 8, loss = 0.50367636\n",
      "Iteration 9, loss = 0.47140838\n",
      "Iteration 10, loss = 0.43733561\n",
      "Iteration 11, loss = 0.40177655\n",
      "Iteration 12, loss = 0.36550256\n",
      "Iteration 13, loss = 0.32892263\n",
      "Iteration 14, loss = 0.29260392\n",
      "Iteration 15, loss = 0.25714077\n",
      "Iteration 16, loss = 0.22307336\n",
      "Iteration 17, loss = 0.19102697\n",
      "Iteration 18, loss = 0.16147059\n",
      "Iteration 19, loss = 0.13458109\n",
      "Iteration 20, loss = 0.11075820\n",
      "Iteration 21, loss = 0.09007257\n",
      "Iteration 22, loss = 0.07244212\n",
      "Iteration 23, loss = 0.05780148\n",
      "Iteration 24, loss = 0.04582880\n",
      "Iteration 25, loss = 0.03621006\n",
      "Iteration 26, loss = 0.02856020\n",
      "Iteration 27, loss = 0.02252354\n",
      "Iteration 28, loss = 0.01780056\n",
      "Iteration 29, loss = 0.01411686\n",
      "Iteration 30, loss = 0.01125537\n",
      "Iteration 31, loss = 0.00904044\n",
      "Iteration 32, loss = 0.00731047\n",
      "Iteration 33, loss = 0.00595523\n",
      "Iteration 34, loss = 0.00488969\n",
      "Iteration 35, loss = 0.00404744\n",
      "Iteration 36, loss = 0.00338035\n",
      "Iteration 37, loss = 0.00285127\n",
      "Iteration 38, loss = 0.00243023\n",
      "Iteration 39, loss = 0.00209265\n",
      "Iteration 40, loss = 0.00182021\n",
      "Iteration 41, loss = 0.00159853\n",
      "Iteration 42, loss = 0.00141705\n",
      "Iteration 43, loss = 0.00126818\n",
      "Iteration 44, loss = 0.00114491\n",
      "Iteration 45, loss = 0.00104193\n",
      "Iteration 46, loss = 0.00095544\n",
      "Iteration 47, loss = 0.00088234\n",
      "Iteration 48, loss = 0.00082013\n",
      "Iteration 49, loss = 0.00076681\n",
      "Iteration 50, loss = 0.00072095\n",
      "Iteration 51, loss = 0.00068132\n",
      "Iteration 52, loss = 0.00064699\n",
      "Iteration 53, loss = 0.00061724\n",
      "Iteration 54, loss = 0.00059135\n",
      "Iteration 55, loss = 0.00056859\n",
      "Iteration 56, loss = 0.00054851\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Time needed for a 100th (137 samples): 1.7941341400146484 s\n",
      "Time needed for the whole dataset (137879 samples): 179.41341400146484 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# classifier = (verbose=1, solver='lbfgs', max_iter=10000)\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(256,128,64), verbose=True)\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "components = [\n",
    "    (\"cleaner\", predictors()),\n",
    "    (\"vectorizer\", bow_vector),\n",
    "    (\"classifier\", classifier)\n",
    "        ]\n",
    "pipe = Pipeline(components)\n",
    "\n",
    "# Test with 1/100 of the data to estimate the time needed\n",
    "before = time.time()\n",
    "pipe.fit(X_train[:len(X_train)//100], y_train[:len(y_train)//100])\n",
    "after = time.time()\n",
    "print(f'\\n\\nTime needed for a 100th ({len(X_train)//100} samples): {after-before} s')\n",
    "print(f'Time needed for the whole dataset ({len(X_train)} samples): {(after-before)*100} s\\n\\n')\n",
    "\n",
    "# Model generation\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cleaning text:\t: 100%|██████████| 1000/1000 [00:00<00:00, 1006794.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.733\n",
      "Logistic Regression Precision: 0.7376425855513308\n",
      "Logistic Regression Recall: 0.7504835589941973\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Predicting with test dataset\n",
    "predicted = pipe.predict(X_test[:1000])\n",
    "\n",
    "# Model accuracy score\n",
    "print(f'Logistic Regression Accuracy: {metrics.accuracy_score(y_test[:1000], predicted)}')\n",
    "print(f'Logistic Regression Precision: {metrics.precision_score(y_test[:1000], predicted)}')\n",
    "print(f'Logistic Regression Recall: {metrics.recall_score(y_test[:1000], predicted)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cil-sentiment-pXwVpgIY",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
